{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_pd = pd.read_csv(\"C:\\Users\\Shyamal\\YT Comments\\comments.csv\")\n",
    "Df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rev_list = Df_pd[\"Comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in (text[:]): \n",
    "        new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punc.\n",
    "        new_text = re.sub(r'\\d+','',new_text)# remove numbers\n",
    "        new_text = new_text.lower() # lower case, .upper() for upper          \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_w(words):\n",
    "    w_new = []\n",
    "    for w in (words[:]):\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            w_new.append(w_token)\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(words):\n",
    "    new = []\n",
    "    for i in range(len(Rev_list)):\n",
    "        lem_words = [lemmatizer.lemmatize(x) for x in (words[:][i])]\n",
    "        new.append(lem_words)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = preprocess(Rev_list) #removes punctuation, see above\n",
    "clean_words = tokenization_w(clean_list) # word tokenization\n",
    "lem = lemmatization(clean_words)\n",
    "lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comment = []\n",
    "for i in lem:\n",
    "    new_comment.append(\" \".join(i))\n",
    "new_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'comment':new_comment})\n",
    "data['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_count'] = data['comment'].apply(lambda x: len(str(x).split(\" \")))\n",
    "data[['comment','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['char_count'] = data['review'].str.len() ## this also includes spaces\n",
    "data[['comment','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Word Length\n",
    "#Number of characters(without space count)/Total number of words\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  print(words)\n",
    "  print(len(words))\n",
    "  print(sum(len(word) for word in words))\n",
    "  if words == []:\n",
    "    return 0\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "data['avg_word'] = data['comment'].apply(lambda x: avg_word(x))\n",
    "data[['comment','avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "data['stopwords'] = data['comment'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "data[['comment','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of special characters\n",
    "data['hastags'] = data['comment'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "data[['comment','hastags']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of numerics\n",
    "data['numerics'] = data['comment'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "data[['comment','numerics']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Uppercase words\n",
    "data['upper'] = data['comment'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "data[['comment','upper']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "                print(ppo, tup)\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "data['noun_count'] = data['comment'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "data['verb_count'] = data['comment'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "data['adj_count'] = data['comment'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "data['adv_count'] = data['comment'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "data['pron_count'] = data['comment'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
    "data[['comment','noun_count','verb_count','adj_count', 'adv_count', 'pron_count' ]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()\n",
    "A_vec = cv.fit_transform(new_review)\n",
    "print(A_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv=TfidfVectorizer()\n",
    "t_vec = tv.fit_transform(new_review)\n",
    "print(t_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tv.get_feature_names()\n",
    "\n",
    "dense = t_vec.todense()\n",
    "denselist = dense.tolist()\n",
    "daf = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c =pd.concat([daf,data], axis=1)\n",
    "df_c.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
